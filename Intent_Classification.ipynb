{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIm13b001zBy"
   },
   "outputs": [],
   "source": [
    "# !pip install pythainlp\n",
    "# !pip install tltk\n",
    "# !pip install emoji\n",
    "# !pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwRg_wgV1w8r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pythainlp\n",
    "import matplotlib as plt\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D,GlobalMaxPooling1D, CuDNNGRU, CuDNNLSTM, TimeDistributed, Flatten,Reshape,SpatialDropout1D, GRU, LSTM, Bidirectional,Dropout,BatchNormalization,MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from pythainlp.spell import spell\n",
    "from itertools import groupby\n",
    "import tltk\n",
    "from sklearn.utils import class_weight\n",
    "# from ast import literal_eval\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pythainlp.tag.named_entity import ThaiNameTagger\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import AllKNN,RandomUnderSampler,ClusterCentroids\n",
    "import pickle \n",
    "ner = ThaiNameTagger()\n",
    "    \n",
    "def inputData(filename):\n",
    "   '''\n",
    "    Should implement for your dataset\n",
    "   '''\n",
    "    file = open(filename,'r')\n",
    "    data = np.array([e.split('|') for e in file.read().split('\\n')])\n",
    "    print('size of train '+str(data.shape))\n",
    "    temp =[]\n",
    "    for i in data :\n",
    "            if len(i) == 2:\n",
    "                temp.append([i[0],i[1]])\n",
    "\n",
    "    temp\n",
    "    data = temp\n",
    "    del temp\n",
    "    data[0]\n",
    "    df = pd.DataFrame(data=data, columns=['input','output'])\n",
    "    return preClean(df)\n",
    "\n",
    "def preClean (df):\n",
    "  '''\n",
    "    Should implement for your dataset\n",
    "  '''\n",
    "    df = df[df.output != '']\n",
    "    df = df[~df.input.str.contains('<->')]\n",
    "    df = df.drop_duplicates(subset=(['input']),keep='first')\n",
    "    s = df.groupby(\"output\").input.agg(lambda x:len(x.unique()))\n",
    "    ClassNum = np.concatenate((np.array(s.index.values).reshape((len(s.index.values),1)),\n",
    "                    np.array(s.values).reshape((len(s.index.values),1))),axis=1)\n",
    "    ClassNum = pd.DataFrame(ClassNum,columns=[\"output\",\"count\"])\n",
    "    df = df[df[\"output\"].isin(ClassNum[ClassNum[\"count\"]>1000].output)]\n",
    "    df = df[df[\"output\"]!=\"No\"]\n",
    "    df = df[df[\"output\"]!=\"Yes\"]\n",
    "    df = df[df[\"output\"]!=\"nomatch\"]\n",
    "    return df\n",
    "\n",
    "def tokenize(text): \n",
    "  '''\n",
    "    tokenization\n",
    "  '''\n",
    "#     return word_tokenize(text,engine='newmm')\n",
    "    return text.split()\n",
    "\n",
    "def wordPreProcess(s):\n",
    "  '''\n",
    "    delete non alphabet except -?, empty string, and replace some entities with entity's name \n",
    "  '''\n",
    "    out=[]\n",
    "    th = [str(chr(a+ord('ก'))) for a in range(91)] \n",
    "    for w in s: \n",
    "        ss=''\n",
    "        for i in range(len(w)):\n",
    "            if w[i] in th or w[i].isalpha() or w[i] in '-?':\n",
    "                ss+=w[i]\n",
    "        if ss.strip() is not '' and len(ss.strip())>=2:\n",
    "            if ner.get_ner(ss.strip())[0][2] in ['B-LOCATION', 'B-TIME', 'B-ORGANIZATION', 'B-DATE', 'B-PERSON']:\n",
    "                out.append(ner.get_ner(ss.strip())[0][2])\n",
    "            else:\n",
    "                out.append(ss.strip())\n",
    "    return out\n",
    "\n",
    "def dataSplit(x,y,mode=\"Test\"):\n",
    "  '''\n",
    "    split data \n",
    "    x is input feature\n",
    "    y is output class\n",
    "    mode = Train is used when you have no test dataset\n",
    "    mode = Test is used when you have test dataset\n",
    "  '''\n",
    "    if mode == \"Train\":\n",
    "        x_train, x_tmp, y_train, y_tmp = train_test_split(x, y, test_size=0.3, \n",
    "                                                        random_state=72,stratify = y)\n",
    "        x_val, x_test, y_val, y_test = train_test_split(x_tmp, y_tmp, test_size=0.3, random_state=72,stratify = y_tmp)\n",
    "    elif mode == \"Test\":\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, \n",
    "                                                        random_state=72,stratify = y)\n",
    "        x_test,y_test = np.array([]),np.array([])\n",
    "    print(\"x_train shape:\",x_train.shape)\n",
    "    print(\"x_val shape:\",x_val.shape)\n",
    "    print(\"x_test shape:\",x_test.shape)\n",
    "    print(\"y_train shape:\",y_train.shape)\n",
    "    print(\"y_val shape:\",y_val.shape)\n",
    "    print(\"y_test shape:\",y_test.shape)\n",
    "    return ((x_train,y_train),(x_val,y_val),(x_test,y_test))\n",
    "\n",
    "def imbDataSplit(x,y,mode=\"Under\"):\n",
    "  '''\n",
    "    split data \n",
    "    x is input feature\n",
    "    y is output class\n",
    "    mode = Under is undersampling\n",
    "    mode = Over is oversampling\n",
    "    mode = Combine is both undersampling and oversampling\n",
    "    defult split mode is Test\n",
    "  '''\n",
    "    if mode ==\"Combine\":\n",
    "        sme = SMOTETomek(random_state=42)\n",
    "        x_res, y_res = sme.fit_resample(x, y)\n",
    "    elif mode ==\"Under\":\n",
    "        allknn = AllKNN(random_state=42)\n",
    "#     allknn = RandomUnderSampler(random_state=42)\n",
    "        x_res, y_res = allknn.fit_resample(x, y)\n",
    "    elif mode ==\"Over\" :\n",
    "#     sm = SMOTE(random_state=42)\n",
    "        sm = RandomOverSampler(random_state=42)\n",
    "        x_res, y_res = sm.fit_resample(x, y)\n",
    "    return dataSplit(x_res,y_res,\"Test\") \n",
    "\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "\n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "\n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "\n",
    "    weights = K.variable(weights)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "'''\n",
    "Compatible with tensorflow backend\n",
    "'''\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "class Models:   \n",
    "    \n",
    "    def __init__(self, trainfile):\n",
    "        train_df = inputData(trainfile)\n",
    "        print(\"\\nInput Data Completed\\n\")\n",
    "\n",
    "        self.mmap = {k : v for k,v in zip(train_df[\"output\"].unique(),range(len(train_df[\"output\"].unique())))}\n",
    "        self.rmap = {v : k for k,v in zip(train_df[\"output\"].unique(),range(len(train_df[\"output\"].unique())))}\n",
    "        input_df,output_df = self.prepare(train_df)\n",
    "        print(\"\\nPrepare Completed\\n\")\n",
    "\n",
    "        self.createDict(input_df)\n",
    "        x = self.sentenceToVector(input_df)\n",
    "        print(\"\\nSentence to vector Completed\\n\")\n",
    "\n",
    "        y = to_categorical(output_df)\n",
    "        print(\"\\nto categorical Completed\\n\")\n",
    "\n",
    "        # ((x_train,y_train),(x_val,y_val),(x_test,y_test)) = imbDataSplit(x,y,\"Under\")\n",
    "        # ((x_train,y_train),(x_val,y_val),_) = imbDataSplit(x,y)\n",
    "        ((x_train,y_train),(x_val,y_val),_) = dataSplit(x,y)\n",
    "        print(\"\\nSplit data Completed\\n\")\n",
    "\n",
    "#         input_all = pd.concat([input_df,test_input])\n",
    "        self.createEmbeddingMatrix(input_df,\"Word2Vec\")\n",
    "        print(\"\\nCreate embedding matrix Completed\\n\")\n",
    "\n",
    "        self.classNum(output_df)\n",
    "        self.classWeight(output_df)\n",
    "        print(\"\\nclassnum&classweight Completed\\n\")\n",
    "\n",
    "        self.model = self.GRUnn()\n",
    "        self.model.fit(x=x_train, y=y_train, batch_size=8, verbose=1, epochs=3, \\\n",
    "                        class_weight=self.class_weights, \n",
    "                        validation_data=(x_val,y_val))\n",
    "        print(\"\\nTrain Completed\\n\")\n",
    "\n",
    "    def evaluate(self,testfile):\n",
    "        test_df = inputData(testfile)\n",
    "        print(\"\\nInput Data Completed\\n\")\n",
    "        \n",
    "        test_input,test_output = self.prepare(test_df)\n",
    "        print(\"\\nPrepare Completed\\n\")\n",
    "        \n",
    "        x_test = self.sentenceToVector(test_input)\n",
    "        print(\"\\nSentence to vector Completed\\n\")\n",
    "        \n",
    "        y_test = to_categorical(test_output)\n",
    "        print(\"\\nto categorical Completed\\n\")\n",
    "        \n",
    "        self.EvalByClass(x_test,y_test)\n",
    "        self.Eval(x_test,y_test)\n",
    "        print(\"\\nEvaluate Completed\\n\")\n",
    "        \n",
    "    def predict(self,sentence):\n",
    "        x_df = self.textPreProcess(pd.DataFrame([sentence],columns=['input']))\n",
    "        x = self.sentenceToVector(x_df)\n",
    "        y = self.model.predict(x)\n",
    "#         y_pred = np.argmax(y,axis=1)\n",
    "        y_df = pd.DataFrame([[np.argmax(y),np.max(y)*100]],columns=['output','confidence'])\n",
    "        return self.categoricalToNumber(y_df,Reverse = True).values[0],np.max(y)*100\n",
    "\n",
    "    def createDict(self,train_df):\n",
    "      '''\n",
    "        Create custom dict to index from dataset\n",
    "      '''\n",
    "        self.max_sentence_len = 0\n",
    "        self.word2index = {\"keras_mask_zero\" : 0}\n",
    "        self.index2word = {0 : \"keras_mask_zero\"}\n",
    "\n",
    "\n",
    "        for s in train_df.values:\n",
    "            self.max_sentence_len = max(len(s),self.max_sentence_len)\n",
    "            for w in s:\n",
    "                if w not in self.word2index:\n",
    "                    self.word2index[w] = len(self.word2index)\n",
    "                    self.index2word[len(self.index2word)] = w\n",
    "        self.word2index[\"UNK\"] = len(self.word2index)\n",
    "        self.index2word[len(self.index2word)] = \"UNK\"\n",
    "        self.vocab_size = len(self.word2index)\n",
    "        print(\"vocab size :\",self.vocab_size)\n",
    "        print(\"max sentence length :\",self.max_sentence_len)\n",
    "        \n",
    "    def textPreProcess(self,df):\n",
    "        '''\n",
    "        Pre-Process text from dataset\n",
    "        '''\n",
    "        input_df = df['input']       \n",
    "        input_df = input_df.apply(tokenize)\n",
    "        input_df = input_df.apply(wordPreProcess)\n",
    "        return input_df\n",
    "\n",
    "\n",
    "    def prepare(self,df):\n",
    "        '''\n",
    "        Pre-Process all dataset\n",
    "        '''      \n",
    "        return self.textPreProcess(df),self.categoricalToNumber(df)\n",
    "\n",
    "    def sentenceToVector(self,df):\n",
    "        '''\n",
    "        tranform sentence to vector\n",
    "        ''' \n",
    "        X = []\n",
    "        for s in df.values:\n",
    "            X.append(s)\n",
    "\n",
    "        x = []\n",
    "        for sentence in X:\n",
    "            tmp = []\n",
    "            for w in sentence:\n",
    "                if w in self.word2index:\n",
    "                    tmp.append(self.word2index[w])\n",
    "                else:\n",
    "                    tmp.append(self.word2index[\"UNK\"])\n",
    "            while(len(tmp) < self.max_sentence_len):\n",
    "                tmp.append(0)\n",
    "            x.append(tmp)\n",
    "        return np.array(x)\n",
    "    def categoricalToNumber(self,df,Reverse = False):\n",
    "        '''\n",
    "        switch between class name and class index\n",
    "        Reverse = False for class name to class index\n",
    "        Reverse = True for class index to class name\n",
    "        ''' \n",
    "        if not Reverse :\n",
    "            return df[\"output\"].map(self.mmap)\n",
    "        else:\n",
    "            return df[\"output\"].map(self.rmap)\n",
    "        return df\n",
    "    def createEmbeddingMatrix(self,input_df,mode):\n",
    "        '''\n",
    "        create embedding matrix \n",
    "        mode = Word2Vec, FastText \n",
    "        ''' \n",
    "        self.WV_size = 300\n",
    "        if mode == \"Word2Vec\":\n",
    "            self.model_embed = Word2Vec(input_df.values, size =self.WV_size , window=10, min_count=1, workers=10)\n",
    "            self.model_embed.train(input_df.values, total_examples=len(input_df.values), epochs=10, compute_loss=True)\n",
    "        elif mode == \"FastText\":\n",
    "            self.model_embed = FastText(size=self.WV_size, window=10, min_count=1, sentences=input_df.values, iter=10)\n",
    "\n",
    "        self.embedding_matrix = zeros((len(self.model_embed.wv.vocab)+1, self.WV_size))\n",
    "        i = 1\n",
    "        for word in self.model_embed.wv.vocab :\n",
    "            embedding_vector = self.model_embed[word]\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "            i+=1\n",
    "\n",
    "    def classNum(self,output_df):\n",
    "        '''\n",
    "        count number of class\n",
    "        ''' \n",
    "        self.classnum = len(output_df.unique())\n",
    "        print(\"Class Number: \",self.classnum)\n",
    "\n",
    "    def classWeight(self,output_df):\n",
    "        '''\n",
    "        compute class weight from dataset\n",
    "        ''' \n",
    "        self.class_weights = class_weight.compute_class_weight('balanced',np.unique(output_df),output_df)\n",
    "        plt.bar([x for x in range(len(self.class_weights))], self.class_weights)\n",
    "        print(self.class_weights)\n",
    "\n",
    "    def FeedForword(self):\n",
    "        '''\n",
    "        FeedForward model\n",
    "        ''' \n",
    "        inp = Input(shape=(self.max_sentence_len,))\n",
    "        x = Embedding(len(self.model_embed.wv.vocab)+1, self.WV_size, weights = [self.embedding_matrix], \n",
    "                      input_length=self.max_sentence_len, trainable=True)(inp)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(200,activation = 'relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        x = Dense(self.classnum, activation = 'softmax')(x)\n",
    "        model = Model(inputs = inp , outputs = x)\n",
    "        model.compile(Adam(), loss=[focal_loss(alpha=.25, gamma=2)], metrics=['acc'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def Conv(self):\n",
    "        '''\n",
    "        Convolutional Model\n",
    "        ''' \n",
    "        inp = Input(shape=(self.max_sentence_len,))\n",
    "        x = Embedding(len(self.model_embed.wv.vocab)+1, self.WV_size, weights = [self.embedding_matrix], \n",
    "                      input_length=self.max_sentence_len, trainable=True)(inp)\n",
    "\n",
    "        x = SpatialDropout1D(0.4)(x)\n",
    "        x = Conv1D(8,5,strides=1,activation='relu')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dense(200,activation = 'relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        x = Dense(self.classnum, activation = 'softmax')(x)\n",
    "        model = Model(inputs = inp , outputs = x)\n",
    "        model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def GRUnn(self):\n",
    "        '''\n",
    "        Recurrent Model\n",
    "        '''\n",
    "        inp = Input(shape=(self.max_sentence_len,))\n",
    "        x = Embedding(len(self.model_embed.wv.vocab)+1, self.WV_size, weights = [self.embedding_matrix], \n",
    "                      input_length=self.max_sentence_len, mask_zero=False, trainable=True)(inp)\n",
    "    #     x = CuDNNGRU(self.WV_size,return_sequences=False, return_state=False)(x)\n",
    "        x = Bidirectional(CuDNNGRU(self.WV_size,return_sequences=False, return_state=False))(x)\n",
    "    #     x = CuDNNLSTM(self.WV_size,return_sequences=False, return_state=False)(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dense(50,activation = 'relu')(x)\n",
    "        x = Dense(200,activation = 'relu')(x)\n",
    "        x = Dense(50,activation = 'relu')(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dense(50,activation = 'relu')(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dense(self.classnum, activation = 'softmax')(x)\n",
    "        model = Model(inputs = inp , outputs = x)\n",
    "        model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def Conv_GRU_nn(self):\n",
    "        '''\n",
    "        Convolutional and Recurrent Model\n",
    "        '''\n",
    "        inp = Input(shape=(self.max_sentence_len,))\n",
    "        x = Embedding(len(self.model_embed.wv.vocab)+1, self.WV_size, weights = [self.embedding_matrix], \n",
    "                      input_length=self.max_sentence_len, trainable=True)(inp)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Conv1D(8,5,strides=1,padding='valid',activation='relu')(x)\n",
    "        x = TimeDistributed(Dense(5, activation = 'relu'))(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(self.max_sentence_len*self.WV_size,activation = 'relu')(x)\n",
    "\n",
    "        x = Reshape((self.max_sentence_len, self.WV_size))(x)\n",
    "\n",
    "\n",
    "        x = Bidirectional(CuDNNGRU(self.WV_size,return_sequences=False, return_state=False))(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dense(50,activation = 'relu')(x)\n",
    "        x = Dense(200,activation = 'relu')(x)\n",
    "        x = Dense(50,activation = 'relu')(x)\n",
    "        x = Dense(100,activation = 'relu')(x)\n",
    "        x = Dense(self.classnum, activation = 'softmax')(x)\n",
    "        model = Model(inputs = inp , outputs = x)\n",
    "        model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
    "        model.summary()\n",
    "        return model\n",
    "        \n",
    "    def Eval(self,x_test,y_test):\n",
    "        y_pred = self.model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred,axis=1)\n",
    "        y_real = np.argmax(y_test,axis=1)\n",
    "\n",
    "        print(\"f1_score  : \",f1_score(y_real,y_pred,average=\"macro\"))\n",
    "        print(\"accuracy  : \",accuracy_score(y_real,y_pred))\n",
    "        print(\"precision : \",precision_score(y_real,y_pred,average=\"macro\"))\n",
    "        print(\"recall    : \",recall_score(y_real,y_pred,average=\"macro\"))\n",
    "\n",
    "    def EvalByClass(self,x_test,y_test):\n",
    "        y_pred = self,model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred,axis=1)\n",
    "        y_real = np.argmax(y_test,axis=1)\n",
    "        y_pred_c = [list() for i in range(classnum)]\n",
    "        y_real_c = [list() for i in range(classnum)]\n",
    "        for i in range(len(y_pred)):\n",
    "                y_pred_c[y_real[i]].append(y_pred[i])\n",
    "                y_real_c[y_real[i]].append(y_real[i])\n",
    "        y_pred_c = np.array(y_pred_c)\n",
    "        y_real_c = np.array(y_real_c)\n",
    "        print(\"0\",\"acc\",\"f1\",\"precision\",\"recall\")\n",
    "        score =[]\n",
    "        for i in range(classnum):\n",
    "            if y_pred_c[i] != []:\n",
    "                score.append([accuracy_score(y_pred_c[i],y_real_c[i]),\n",
    "                f1_score(y_pred_c[i],y_real_c[i],average='micro'),\n",
    "                precision_score(y_pred_c[i],y_real_c[i],average='micro'),\n",
    "                recall_score(y_pred_c[i],y_real_c[i],average='micro')])\n",
    "        score = np.array(score)\n",
    "        print(\"all\",score.mean(axis=0))\n",
    "        print(classification_report_imbalanced(y_real, y_pred))\n",
    "    def save(self,filename):\n",
    "        with open(filename+\".model\", 'wb') as modelfile:\n",
    "            pickle.dump(self, modelfile)\n",
    "    def load(filename):\n",
    "        with open(filename+\".model\", 'rb') as modelfile:\n",
    "            tmp = pickle.load(modelfile)\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKhrBU6AZjFE"
   },
   "outputs": [],
   "source": [
    "# path = modelPath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4vYbDYIyDrG"
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict/<sentence>')\n",
    "def predict(sentence):\n",
    "    global path\n",
    "    K.clear_session()\n",
    "    model = Models.load(path)\n",
    "    y = model.predict(sentence)\n",
    "    K.clear_session()\n",
    "    return \"action : \" + y[0] + \"<br>confidence : \" + str(y[1])\n",
    "\n",
    "run_with_ngrok(app)\n",
    "if __name__  == '__main__':\n",
    "  app.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Intent Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
